#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›†æˆç®¡é“ï¼šPDFå¤„ç† + å‘é‡åŒ–å­˜å‚¨
"""

import os
import json
import argparse
from datetime import datetime
from typing import List, Dict, Any
from .pdf_processor import ChemicalPDFProcessor
from .vector_store import ChemicalVectorStore

class IntegratedPipeline:
    """é›†æˆç®¡é“ï¼šä»PDFåˆ°å‘é‡æ•°æ®åº“çš„å®Œæ•´æµç¨‹"""
    
    def __init__(self, 
                 pdf_directory: str = "data/pdf",
                 vector_db_path: str = "data/vector_db",
                 collection_name: str = "chemical_documents"):
        """
        åˆå§‹åŒ–é›†æˆç®¡é“
        
        Args:
            pdf_directory: PDFæ–‡ä»¶ç›®å½•
            vector_db_path: å‘é‡æ•°æ®åº“è·¯å¾„
            collection_name: é›†åˆåç§°
        """
        self.pdf_directory = pdf_directory
        self.vector_db_path = vector_db_path
        self.collection_name = collection_name
        
        # åˆå§‹åŒ–å‘é‡å­˜å‚¨ç³»ç»Ÿ
        self.vector_store = ChemicalVectorStore(
            persist_directory=vector_db_path,
            collection_name=collection_name
        )
        
        # å¤„ç†çŠ¶æ€è·Ÿè¸ª
        self.processed_files = set()
        self.status_file = os.path.join(vector_db_path, "processing_status.json")
        self.load_processing_status()
    
    def load_processing_status(self):
        """åŠ è½½å¤„ç†çŠ¶æ€"""
        try:
            if os.path.exists(self.status_file):
                with open(self.status_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.processed_files = set(data.get('processed_files', []))
                print(f"ğŸ“‹ åŠ è½½å¤„ç†çŠ¶æ€: {len(self.processed_files)} ä¸ªæ–‡ä»¶å·²å¤„ç†")
            else:
                self.processed_files = set()
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å¤„ç†çŠ¶æ€å¤±è´¥: {e}")
            self.processed_files = set()
    
    def save_processing_status(self):
        """ä¿å­˜å¤„ç†çŠ¶æ€"""
        try:
            os.makedirs(os.path.dirname(self.status_file), exist_ok=True)
            with open(self.status_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'processed_files': list(self.processed_files),
                    'last_updated': str(datetime.now())
                }, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å¤„ç†çŠ¶æ€å¤±è´¥: {e}")
    
    def is_file_processed(self, pdf_path: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†"""
        filename = os.path.basename(pdf_path)
        return filename in self.processed_files
    
    def mark_file_processed(self, pdf_path: str):
        """æ ‡è®°æ–‡ä»¶ä¸ºå·²å¤„ç†"""
        filename = os.path.basename(pdf_path)
        self.processed_files.add(filename)
        self.save_processing_status()
    
    def get_json_path(self, pdf_path: str) -> str:
        """è·å–å¯¹åº”çš„JSONæ–‡ä»¶è·¯å¾„"""
        base_name = os.path.splitext(os.path.basename(pdf_path))[0]
        return os.path.join(os.path.dirname(pdf_path), f"{base_name}_structured.json")
    
    def check_file_freshness(self, pdf_path: str, json_path: str) -> bool:
        """æ£€æŸ¥PDFæ–‡ä»¶æ˜¯å¦æ¯”JSONæ–‡ä»¶æ›´æ–°"""
        try:
            pdf_mtime = os.path.getmtime(pdf_path)
            json_mtime = os.path.getmtime(json_path)
            return pdf_mtime > json_mtime
        except:
            return True  # å¦‚æœæ— æ³•æ¯”è¾ƒï¼Œè®¤ä¸ºéœ€è¦é‡æ–°å¤„ç†
    
    def process_pdf_to_vectors(self, pdf_path: str, force_reprocess: bool = False) -> bool:
        """
        å¤„ç†å•ä¸ªPDFæ–‡ä»¶ï¼šæå–å†…å®¹ -> ç”ŸæˆJSON -> å‘é‡åŒ–å­˜å‚¨
        
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            force_reprocess: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¤„ç†
        
        Returns:
            æ˜¯å¦å¤„ç†æˆåŠŸ
        """
        try:
            filename = os.path.basename(pdf_path)
            json_path = self.get_json_path(pdf_path)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¤„ç†
            if not force_reprocess:
                if self.is_file_processed(pdf_path):
                    if os.path.exists(json_path):
                        # æ£€æŸ¥æ–‡ä»¶æ–°é²œåº¦
                        if not self.check_file_freshness(pdf_path, json_path):
                            print(f"â­ï¸ è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {filename}")
                            # æ£€æŸ¥æ˜¯å¦å·²åŠ è½½åˆ°å‘é‡æ•°æ®åº“
                            if not self.vector_store.is_document_exists(json_path):
                                print(f"ğŸ”„ åŠ è½½åˆ°å‘é‡æ•°æ®åº“: {filename}")
                                success = self.vector_store.add_documents(json_path)
                                return success
                            return True
                        else:
                            print(f"ğŸ”„ PDFæ–‡ä»¶å·²æ›´æ–°ï¼Œé‡æ–°å¤„ç†: {filename}")
                    else:
                        print(f"âš ï¸ æ ‡è®°ä¸ºå·²å¤„ç†ä½†JSONæ–‡ä»¶ä¸å­˜åœ¨ï¼Œé‡æ–°å¤„ç†: {filename}")
                else:
                    print(f"ğŸ†• æ–°æ–‡ä»¶ï¼Œå¼€å§‹å¤„ç†: {filename}")
            else:
                print(f"ğŸ”„ å¼ºåˆ¶é‡æ–°å¤„ç†: {filename}")
            
            print(f"\nğŸ”„ å¼€å§‹å¤„ç†PDF: {pdf_path}")
            
            # 1. PDFå¤„ç†
            print("1ï¸âƒ£ æå–PDFå†…å®¹...")
            processor = ChemicalPDFProcessor(pdf_path)
            result = processor.process_full_document()
            
            if not result:
                print("âŒ PDFå¤„ç†å¤±è´¥")
                return False
            
            # 2. ä¿å­˜JSONæ–‡ä»¶
            print("2ï¸âƒ£ ä¿å­˜ç»“æ„åŒ–æ•°æ®...")
            json_path = processor.save_structured_data()
            
            # 3. å‘é‡åŒ–å­˜å‚¨
            print("3ï¸âƒ£ å‘é‡åŒ–å­˜å‚¨...")
            success = self.vector_store.add_documents(json_path, force_reload=force_reprocess)
            
            if success:
                print(f"âœ… æˆåŠŸå¤„ç†: {pdf_path}")
                # æ ‡è®°ä¸ºå·²å¤„ç†
                self.mark_file_processed(pdf_path)
                return True
            else:
                print(f"âŒ å‘é‡åŒ–å­˜å‚¨å¤±è´¥: {pdf_path}")
                return False
                
        except Exception as e:
            print(f"âŒ å¤„ç†PDFæ—¶å‡ºé”™: {e}")
            return False
    
    def process_all_pdfs(self, force_reprocess: bool = False) -> Dict[str, bool]:
        """
        å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰PDFæ–‡ä»¶
        
        Args:
            force_reprocess: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¤„ç†æ‰€æœ‰æ–‡ä»¶
        
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        results = {}
        
        if not os.path.exists(self.pdf_directory):
            print(f"âŒ PDFç›®å½•ä¸å­˜åœ¨: {self.pdf_directory}")
            return results
        
        # æŸ¥æ‰¾æ‰€æœ‰PDFæ–‡ä»¶
        pdf_files = []
        for file in os.listdir(self.pdf_directory):
            if file.lower().endswith('.pdf'):
                pdf_files.append(os.path.join(self.pdf_directory, file))
        
        if not pdf_files:
            print(f"âŒ åœ¨ {self.pdf_directory} ä¸­æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶")
            return results
        
        print(f"ğŸ“ æ‰¾åˆ° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")
        
        # ç»Ÿè®¡éœ€è¦å¤„ç†çš„æ–‡ä»¶
        new_files = []
        updated_files = []
        skipped_files = []
        
        for pdf_path in pdf_files:
            filename = os.path.basename(pdf_path)
            json_path = self.get_json_path(pdf_path)
            
            if force_reprocess:
                new_files.append(pdf_path)
            elif self.is_file_processed(pdf_path):
                if os.path.exists(json_path):
                    if self.check_file_freshness(pdf_path, json_path):
                        updated_files.append(pdf_path)
                    else:
                        skipped_files.append(pdf_path)
                else:
                    new_files.append(pdf_path)
            else:
                new_files.append(pdf_path)
        
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"   ğŸ†• æ–°æ–‡ä»¶: {len(new_files)}")
        print(f"   ğŸ”„ æ›´æ–°æ–‡ä»¶: {len(updated_files)}")
        print(f"   â­ï¸ è·³è¿‡æ–‡ä»¶: {len(skipped_files)}")
        
        # å¤„ç†éœ€è¦å¤„ç†çš„æ–‡ä»¶
        all_process_files = new_files + updated_files
        for pdf_path in all_process_files:
            filename = os.path.basename(pdf_path)
            print(f"\n{'='*60}")
            print(f"å¤„ç†æ–‡ä»¶: {filename}")
            print(f"{'='*60}")
            
            success = self.process_pdf_to_vectors(pdf_path, force_reprocess)
            results[filename] = success
        
        # è®°å½•è·³è¿‡çš„æ–‡ä»¶
        for pdf_path in skipped_files:
            filename = os.path.basename(pdf_path)
            results[filename] = True  # æ ‡è®°ä¸ºæˆåŠŸï¼ˆå·²å­˜åœ¨ï¼‰
        
        return results
    
    def search_documents(self, 
                        query: str, 
                        n_results: int = 5,
                        doc_type: str = None) -> List[Dict[str, Any]]:
        """
        æœç´¢æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: è¿”å›ç»“æœæ•°é‡
            doc_type: æ–‡æ¡£ç±»å‹è¿‡æ»¤
        
        Returns:
            æœç´¢ç»“æœ
        """
        filter_metadata = None
        if doc_type:
            filter_metadata = {"type": doc_type}
        
        return self.vector_store.search(query, n_results, filter_metadata)
    
    def get_database_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®åº“ä¿¡æ¯"""
        return self.vector_store.get_collection_info()
    
    def interactive_search(self):
        """ç®€åŒ–çš„äº¤äº’å¼æœç´¢ç•Œé¢"""
        print("\nğŸ” æœç´¢ç•Œé¢ (è¾“å…¥ 'quit' é€€å‡º)")
        print("-" * 40)
        
        while True:
            try:
                query = input("\næœç´¢: ").strip()
                
                if query.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                if not query:
                    continue
                
                # æ‰§è¡Œæœç´¢
                results = self.search_documents(query, n_results=3)
                
                if not results:
                    print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ")
                    continue
                
                print(f"\næ‰¾åˆ° {len(results)} ä¸ªç»“æœ:")
                for i, result in enumerate(results, 1):
                    similarity = 1 - result['distance']
                    content = result['content'][:100].replace('\n', ' ')
                    print(f"{i}. [{similarity:.2f}] {content}...")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§ï¼")
                break
            except Exception as e:
                print(f"âŒ æœç´¢å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='åŒ–å·¥ä¸“ä¸šæ–‡æ¡£é›†æˆå¤„ç†ç®¡é“')
    parser.add_argument('--force', '-f', action='store_true', 
                       help='å¼ºåˆ¶é‡æ–°å¤„ç†æ‰€æœ‰PDFæ–‡ä»¶')
    parser.add_argument('--no-search', action='store_true',
                       help='å¤„ç†å®Œæˆåä¸å¯åŠ¨äº¤äº’å¼æœç´¢')
    parser.add_argument('--search-only', action='store_true',
                       help='åªè¿›è¡Œæœç´¢ï¼Œä¸å¤„ç†PDFæ–‡ä»¶')
    args = parser.parse_args()
    
    print("ğŸ”¬ åŒ–å·¥ä¸“ä¸šæ–‡æ¡£é›†æˆå¤„ç†ç®¡é“")
    print("=" * 60)
    
    if args.force:
        print("ğŸ”„ å¼ºåˆ¶é‡æ–°å¤„ç†æ¨¡å¼")
    if args.search_only:
        print("ğŸ” ä»…æœç´¢æ¨¡å¼")
    
    # åˆå§‹åŒ–ç®¡é“
    pipeline = IntegratedPipeline()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å·²å¤„ç†çš„JSONæ–‡ä»¶ï¼ˆä»…ç”¨äºé¦–æ¬¡åŠ è½½ï¼‰
    json_files = []
    if os.path.exists("./PDF"):
        for file in os.listdir("./PDF"):
            if file.endswith('_structured.json'):
                json_files.append(os.path.join("./PDF", file))
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é¦–æ¬¡åŠ è½½JSONæ–‡ä»¶åˆ°å‘é‡æ•°æ®åº“
    db_info = pipeline.get_database_info()
    if db_info.get('document_count', 0) == 0 and json_files:
        print(f"ğŸ“ å‘ç° {len(json_files)} ä¸ªå·²å¤„ç†çš„JSONæ–‡ä»¶ï¼Œæ­£åœ¨åŠ è½½åˆ°å‘é‡æ•°æ®åº“...")
        
        # ç›´æ¥åŠ è½½åˆ°å‘é‡æ•°æ®åº“ï¼ˆä¸é‡æ–°å¤„ç†PDFï¼‰
        for json_file in json_files:
            print(f"æ­£åœ¨åŠ è½½: {os.path.basename(json_file)}")
            success = pipeline.vector_store.add_documents(json_file, force_reload=False)
            if success:
                print("âœ… åŠ è½½æˆåŠŸ")
            else:
                print("âŒ åŠ è½½å¤±è´¥")
    
    # å¦‚æœä¸æ˜¯ä»…æœç´¢æ¨¡å¼ï¼Œåˆ™å¤„ç†PDFæ–‡ä»¶
    if not args.search_only:
        # æ™ºèƒ½å¤„ç†PDFæ–‡ä»¶ï¼ˆè‡ªåŠ¨è·³è¿‡å·²å¤„ç†çš„æ–‡ä»¶ï¼‰
        print("\nğŸ”„ å¼€å§‹æ™ºèƒ½å¤„ç†PDFæ–‡ä»¶...")
        results = pipeline.process_all_pdfs(force_reprocess=args.force)
    else:
        # ä»…æœç´¢æ¨¡å¼ï¼Œè·³è¿‡PDFå¤„ç†
        print("\nâ­ï¸ è·³è¿‡PDFå¤„ç†ï¼Œç›´æ¥è¿›å…¥æœç´¢æ¨¡å¼")
        results = {}
    
    # æ˜¾ç¤ºå¤„ç†ç»“æœ
    print("\nğŸ“Š å¤„ç†ç»“æœæ±‡æ€»:")
    print("-" * 40)
    success_count = 0
    processed_count = 0
    skipped_count = 0
    
    for filename, success in results.items():
        if success:
            success_count += 1
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°å¤„ç†çš„æ–‡ä»¶
            if pipeline.is_file_processed(os.path.join("./PDF", filename)):
                processed_count += 1
            else:
                skipped_count += 1
        else:
            print(f"âŒ {filename}: å¤„ç†å¤±è´¥")
    
    print(f"âœ… æˆåŠŸ: {success_count} ä¸ªæ–‡ä»¶")
    print(f"ğŸ†• æ–°å¤„ç†: {processed_count} ä¸ªæ–‡ä»¶")
    print(f"â­ï¸ è·³è¿‡: {skipped_count} ä¸ªæ–‡ä»¶")
    
    # æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯
    info = pipeline.get_database_info()
    print(f"\nğŸ“Š å‘é‡æ•°æ®åº“ä¿¡æ¯: {info}")
    
    # å¯åŠ¨äº¤äº’å¼æœç´¢
    if not args.no_search:
        # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦æœ‰æ•°æ®
        db_info = pipeline.get_database_info()
        if db_info.get('document_count', 0) > 0:
            pipeline.interactive_search()
        else:
            print("âŒ å‘é‡æ•°æ®åº“ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œæœç´¢")
    else:
        print("âœ… å¤„ç†å®Œæˆï¼Œè·³è¿‡äº¤äº’å¼æœç´¢")

if __name__ == "__main__":
    main()
