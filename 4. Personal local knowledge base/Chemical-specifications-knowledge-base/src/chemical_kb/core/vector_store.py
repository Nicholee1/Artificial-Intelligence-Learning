import chromadb
from sentence_transformers import SentenceTransformer
import json
import os
import uuid
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from datetime import datetime
import re

class ChemicalVectorStore:
    """åŒ–å·¥ä¸“ä¸šæ–‡æ¡£å‘é‡åŒ–å­˜å‚¨ç³»ç»Ÿ"""
    
    def __init__(self, 
                 persist_directory: str = "./chroma_db",
                 collection_name: str = "chemical_documents",
                 model_name: str = "all-MiniLM-L6-v2"):
        """
        åˆå§‹åŒ–å‘é‡å­˜å‚¨ç³»ç»Ÿ
        
        Args:
            persist_directory: ChromaDBæŒä¹…åŒ–ç›®å½•
            collection_name: é›†åˆåç§°
            model_name: æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹åç§°
        """
        self.persist_directory = persist_directory
        self.collection_name = collection_name
        self.model_name = model_name
        
        # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯
        self.client = chromadb.PersistentClient(path=persist_directory)
        
        # åˆå§‹åŒ–æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹
        print(f"æ­£åœ¨åŠ è½½æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)
        print("æ¨¡å‹åŠ è½½å®Œæˆ")
        
        # è·å–æˆ–åˆ›å»ºé›†åˆ
        try:
            self.collection = self.client.get_collection(name=collection_name)
            print(f"ä½¿ç”¨ç°æœ‰é›†åˆ: {collection_name}")
        except:
            self.collection = self.client.create_collection(
                name=collection_name,
                metadata={"description": "åŒ–å·¥ä¸“ä¸šæ–‡æ¡£å‘é‡å­˜å‚¨"}
            )
            print(f"åˆ›å»ºæ–°é›†åˆ: {collection_name}")
    
    def load_json_data(self, json_path: str) -> Dict[str, Any]:
        """åŠ è½½PDFå¤„ç†åçš„JSONæ•°æ®"""
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"æˆåŠŸåŠ è½½JSONæ•°æ®: {json_path}")
            return data
        except Exception as e:
            print(f"åŠ è½½JSONæ•°æ®å¤±è´¥: {e}")
            return {}
    
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """
        å°†é•¿æ–‡æœ¬åˆ†å‰²æˆå°å—ï¼Œä¾¿äºå‘é‡åŒ–
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            chunk_size: å—å¤§å°
            overlap: é‡å å¤§å°
        
        Returns:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        if not text or len(text) <= chunk_size:
            return [text] if text else []
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # å°è¯•åœ¨å¥å·ã€æ¢è¡Œç¬¦ç­‰ä½ç½®åˆ†å‰²
            if end < len(text):
                # å¯»æ‰¾åˆé€‚çš„åˆ†å‰²ç‚¹
                for i in range(end, max(start + chunk_size - 100, start), -1):
                    if text[i] in ['ã€‚', '\n', 'ï¼›', 'ï¼', 'ï¼Ÿ', '.', ';', '!', '?']:
                        end = i + 1
                        break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - overlap
            if start >= len(text):
                break
        
        return chunks
    
    def extract_document_chunks(self, json_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        ä»JSONæ•°æ®ä¸­æå–æ–‡æ¡£å—ï¼Œç”¨äºå‘é‡åŒ–
        
        Args:
            json_data: PDFå¤„ç†åçš„JSONæ•°æ®
        
        Returns:
            æ–‡æ¡£å—åˆ—è¡¨
        """
        chunks = []
        doc_id = str(uuid.uuid4())
        metadata = json_data.get('metadata', {})
        
        # 1. å¤„ç†å®Œæ•´æ–‡æœ¬å†…å®¹
        full_text = json_data.get('text_content', '')
        if full_text:
            text_chunks = self.chunk_text(full_text)
            for i, chunk in enumerate(text_chunks):
                chunks.append({
                    'id': f"{doc_id}_text_{i}",
                    'content': chunk,
                    'type': 'text',
                    'source': 'full_text',
                    'metadata': {
                        'document_title': metadata.get('title', ''),
                        'page_count': metadata.get('page_count', 0),
                        'chunk_index': i,
                        'total_chunks': len(text_chunks)
                    }
                })
        
        # 2. å¤„ç†é¡µé¢å†…å®¹
        pages = json_data.get('pages', [])
        for page in pages:
            page_text = page.get('text', '')
            if page_text:
                page_chunks = self.chunk_text(page_text, chunk_size=300)
                for i, chunk in enumerate(page_chunks):
                    chunks.append({
                        'id': f"{doc_id}_page_{page['page_number']}_{i}",
                        'content': chunk,
                        'type': 'page_text',
                        'source': f"page_{page['page_number']}",
                        'metadata': {
                            'document_title': metadata.get('title', ''),
                            'page_number': page['page_number'],
                            'page_width': page.get('width', 0),
                            'page_height': page.get('height', 0),
                            'chunk_index': i
                        }
                    })
        
        # 3. å¤„ç†è¡¨æ ¼æ•°æ®
        tables = json_data.get('tables', [])
        for table in tables:
            # è¡¨æ ¼æ ‡é¢˜å’Œå†…å®¹
            table_content = f"è¡¨æ ¼ {table.get('table_number', '')} (ç¬¬{table.get('page_number', '')}é¡µ)\n"
            
            # æ·»åŠ è¡¨å¤´
            headers = table.get('headers', [])
            if headers:
                table_content += "è¡¨å¤´: " + " | ".join([str(h) for h in headers if h]) + "\n"
            
            # æ·»åŠ è¡¨æ ¼æ•°æ®
            data = table.get('data', [])
            for row in data[:10]:  # åªå–å‰10è¡Œ
                row_text = " | ".join([str(v) for v in row.values() if v])
                if row_text.strip():
                    table_content += row_text + "\n"
            
            if table_content.strip():
                shape = table.get('shape', [3,3])
                shape_str = ','.join(map(str, shape)) 
                chunks.append({
                    'id': f"{doc_id}_table_{table.get('page_number', 0)}_{table.get('table_number', 0)}",
                    'content': table_content.strip(),
                    'type': 'table',
                    'source': f"page_{table.get('page_number', 0)}",
                    'metadata': {
                        'document_title': metadata.get('title', ''),
                        'page_number': table.get('page_number', 0),
                        'table_number': table.get('table_number', 0),
                        'table_shape': shape_str
                    }
                })
        
        # 4. å¤„ç†ç»“æ„åŒ–æ•°æ®ä¸­çš„ç« èŠ‚
        structured_data = json_data.get('structured_data', {})
        sections = structured_data.get('sections', [])
        for i, section in enumerate(sections):
            section_content = f"{section.get('title', '')}\n{section.get('content', '')}"
            if section_content.strip():
                chunks.append({
                    'id': f"{doc_id}_section_{i}",
                    'content': section_content.strip(),
                    'type': 'section',
                    'source': 'structured_data',
                    'metadata': {
                        'document_title': metadata.get('title', ''),
                        'section_title': section.get('title', ''),
                        'section_index': i
                    }
                })
        
        # 5. å¤„ç†æŠ€æœ¯è§„æ ¼
        tech_specs = structured_data.get('technical_specifications', {})
        for spec_type, spec_values in tech_specs.items():
            if spec_values and isinstance(spec_values, list):
                spec_content = f"{spec_type}: " + ", ".join([str(v) for v in spec_values[:20]])
                if spec_content.strip():
                    chunks.append({
                        'id': f"{doc_id}_spec_{spec_type}",
                        'content': spec_content,
                        'type': 'technical_spec',
                        'source': 'technical_specifications',
                        'metadata': {
                            'document_title': metadata.get('title', ''),
                            'spec_type': spec_type,
                            'spec_count': len(spec_values)
                        }
                    })
        
        print(f"æå–äº† {len(chunks)} ä¸ªæ–‡æ¡£å—")
        return chunks
    
    def generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """ç”Ÿæˆæ–‡æœ¬çš„å‘é‡è¡¨ç¤º"""
        try:
            embeddings = self.model.encode(texts)
            return embeddings.tolist()
        except Exception as e:
            print(f"ç”Ÿæˆå‘é‡å¤±è´¥: {e}")
            return []
    
    def is_document_exists(self, json_path: str) -> bool:
        """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²å­˜åœ¨äºå‘é‡æ•°æ®åº“ä¸­"""
        try:
            # ä»JSONè·¯å¾„ç”Ÿæˆæ–‡æ¡£IDå‰ç¼€
            base_name = os.path.splitext(os.path.basename(json_path))[0]
            doc_prefix = base_name.replace('_structured', '')
            
            # è·å–æ‰€æœ‰æ–‡æ¡£ID
            all_docs = self.collection.get()
            existing_ids = all_docs.get('ids', [])
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»¥è¯¥æ–‡æ¡£å‰ç¼€å¼€å¤´çš„ID
            for doc_id in existing_ids:
                if doc_id.startswith(doc_prefix):
                    return True
            return False
        except:
            return False
    
    def add_documents(self, json_path: str, force_reload: bool = False) -> bool:
        """
        å°†PDFå¤„ç†åçš„JSONæ•°æ®æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
        
        Args:
            json_path: JSONæ–‡ä»¶è·¯å¾„
            force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½
        
        Returns:
            æ˜¯å¦æ·»åŠ æˆåŠŸ
        """
        try:
            # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²å­˜åœ¨
            if not force_reload and self.is_document_exists(json_path):
                print(f"â­ï¸ æ–‡æ¡£å·²å­˜åœ¨äºå‘é‡æ•°æ®åº“: {os.path.basename(json_path)}")
                return True
            
            # åŠ è½½JSONæ•°æ®
            json_data = self.load_json_data(json_path)
            if not json_data:
                return False
            
            # æå–æ–‡æ¡£å—
            chunks = self.extract_document_chunks(json_data)
            if not chunks:
                print("æ²¡æœ‰æå–åˆ°æœ‰æ•ˆçš„æ–‡æ¡£å—")
                return False
            
            # å‡†å¤‡æ•°æ®
            texts = [chunk['content'] for chunk in chunks]
            ids = [chunk['id'] for chunk in chunks]
            metadatas = [chunk['metadata'] for chunk in chunks]
            
            # ç”Ÿæˆå‘é‡
            print("æ­£åœ¨ç”Ÿæˆå‘é‡...")
            embeddings = self.generate_embeddings(texts)
            
            if not embeddings:
                print("å‘é‡ç”Ÿæˆå¤±è´¥")
                return False
            
            # æ·»åŠ åˆ°ChromaDB
            print("æ­£åœ¨æ·»åŠ åˆ°å‘é‡æ•°æ®åº“...")
            self.collection.add(
                embeddings=embeddings,
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )
            
            print(f"æˆåŠŸæ·»åŠ  {len(chunks)} ä¸ªæ–‡æ¡£å—åˆ°å‘é‡æ•°æ®åº“")
            return True
            
        except Exception as e:
            print(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def search(self, 
               query: str, 
               n_results: int = 5,
               filter_metadata: Optional[Dict] = None) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            n_results: è¿”å›ç»“æœæ•°é‡
            filter_metadata: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
        
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.generate_embeddings([query])[0]
            
            # æ‰§è¡Œæœç´¢
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                where=filter_metadata
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            for i in range(len(results['documents'][0])):
                formatted_results.append({
                    'content': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'distance': results['distances'][0][i],
                    'id': results['ids'][0][i]
                })
            
            return formatted_results
            
        except Exception as e:
            print(f"æœç´¢å¤±è´¥: {e}")
            return []
    
    def get_collection_info(self) -> Dict[str, Any]:
        """è·å–é›†åˆä¿¡æ¯"""
        try:
            count = self.collection.count()
            return {
                'collection_name': self.collection_name,
                'document_count': count,
                'persist_directory': self.persist_directory
            }
        except Exception as e:
            print(f"è·å–é›†åˆä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def delete_document(self, document_id: str) -> bool:
        """åˆ é™¤æŒ‡å®šæ–‡æ¡£"""
        try:
            self.collection.delete(ids=[document_id])
            print(f"æˆåŠŸåˆ é™¤æ–‡æ¡£: {document_id}")
            return True
        except Exception as e:
            print(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return False
    
    def clear_collection(self) -> bool:
        """æ¸…ç©ºé›†åˆ"""
        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£ID
            all_docs = self.collection.get()
            if all_docs['ids']:
                self.collection.delete(ids=all_docs['ids'])
            print("æˆåŠŸæ¸…ç©ºé›†åˆ")
            return True
        except Exception as e:
            print(f"æ¸…ç©ºé›†åˆå¤±è´¥: {e}")
            return False

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå‘é‡åŒ–å­˜å‚¨ç³»ç»Ÿçš„ä½¿ç”¨"""
    print("ğŸ”¬ åŒ–å·¥ä¸“ä¸šæ–‡æ¡£å‘é‡åŒ–å­˜å‚¨ç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆå§‹åŒ–å‘é‡å­˜å‚¨ç³»ç»Ÿ
    vector_store = ChemicalVectorStore()
    
    # ç¤ºä¾‹ï¼šæ·»åŠ PDFå¤„ç†åçš„JSONæ•°æ®
    json_file = "./PDF/KLDL-03c-04-05PD-B58-2021 ç®¡é“ä¸“ä¸šè¯¦ç»†è®¾è®¡å·¥ç¨‹è®¾è®¡æ–‡ä»¶å†…å®¹å’Œæ·±åº¦ç»Ÿä¸€è§„å®š_structured.json"
    
    if os.path.exists(json_file):
        print(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {json_file}")
        success = vector_store.add_documents(json_file)
        
        if success:
            print("âœ… æ–‡æ¡£æ·»åŠ æˆåŠŸ")
            
            # æ˜¾ç¤ºé›†åˆä¿¡æ¯
            info = vector_store.get_collection_info()
            print(f"ğŸ“Š é›†åˆä¿¡æ¯: {info}")
            
            # ç¤ºä¾‹æœç´¢
            print("\nğŸ” ç¤ºä¾‹æœç´¢:")
            queries = [
                "ç®¡é“è®¾è®¡è§„èŒƒ",
                "è®¾å¤‡å¸ƒç½®å›¾",
                "ææ–™è§„æ ¼è¡¨",
                "è®¾è®¡æ–‡ä»¶ç¼–å·"
            ]
            
            for query in queries:
                print(f"\næŸ¥è¯¢: {query}")
                results = vector_store.search(query, n_results=3)
                for i, result in enumerate(results, 1):
                    print(f"  {i}. {result['content'][:100]}...")
                    print(f"     ç±»å‹: {result['metadata'].get('type', 'unknown')}")
                    print(f"     æ¥æº: {result['metadata'].get('source', 'unknown')}")
        else:
            print("âŒ æ–‡æ¡£æ·»åŠ å¤±è´¥")
    else:
        print(f"âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
        print("è¯·å…ˆè¿è¡ŒPDFå¤„ç†ç¨‹åºç”ŸæˆJSONæ–‡ä»¶")

if __name__ == "__main__":
    main()
